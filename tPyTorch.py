import torch
import torch.nn as nn
import torch.optim as optim
import random

class MatchPennies:
    def __init__(self, n_players):
        self.n_players = n_players
        self.action_space = [0, 1]
        self.payoffs = [1, -1]
        
    def play(self, actions):
        payoff = 0
        for i in range(self.n_players):
            opponent_actions = actions[:i] + actions[i+1:]
            payoff += self.payoffs[actions[i]] * sum(opponent_actions)
        return payoff

class MatchPenniesBRE:
    def __init__(self, n_players, learning_rate=0.1):
        self.n_players = n_players
        self.match_pennies = MatchPennies(n_players)
        self.strategy = nn.Parameter(torch.ones(self.n_players, len(self.match_pennies.action_space)), requires_grad=True)
        self.optimizer = optim.SGD([self.strategy], lr=learning_rate)
        self.probs = None
        
    def _get_action(self, player):
        self.probs = nn.functional.softmax(self.strategy[player], dim=-1)
        action = torch.multinomial(self.probs, 1)
        return action.item()

    def train(self, num_iterations):

        for iteration in range(num_iterations):
            actions = [self._get_action(player) for player in range(self.n_players)]
            payoffs = self.match_pennies.play(actions)
            
            self.optimizer.zero_grad()
            loss = -payoffs
            loss.backward()
            self.optimizer.step()

        exploitability = 0
        num_actions = len(self.match_pennies.action_space) ** self.n_players
        for i in range(num_actions):
            actions = []
            for j in range(self.n_players):
                actions.append((i // len(self.match_pennies.action_space) ** j) % len(self.match_pennies.action_space))
            payoffs = self.match_pennies.play(actions)
            for j in range(self.n_players):
                deviations = torch.zeros_like(self.strategy)
                for action in self.match_pennies.action_space:
                    action_indices = [action] * self.n_players
                    action_indices[j] = slice(None)
                    deviation_actions = actions[:j] + [action] + actions[j+1:]
                    deviation_payoffs = self.match_pennies.play(deviation_actions)
                    deviations[:, action] += torch.exp(deviation_payoffs - payoffs)
                exploitability += self.probs[j][actions[j]] * (deviations.sum() - 1 / num_actions)
                
        return exploitability / self.n_players
    

if __name__ == '__main__':
    match_pennies_bre = MatchPenniesBRE(n_players=2)
    exploitability = match_pennies_bre.train(num_iterations=1000)
    print(exploitability)
