import time
import torch
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
import yaml
import utils
from torch.distributions.beta import Beta

class GlicksbergGrossGame(torch.nn.Module):
    def __init__(self, n_players):
        super(GlicksbergGrossGame, self).__init__()
        self.n_players = n_players
        self.action_dim = 1
        self.action_space = torch.linspace(0, 1, 100)
        self.register_parameter('alpha', torch.nn.Parameter(torch.randn(n_players).abs_().requires_grad_()))
        self.register_parameter('beta', torch.nn.Parameter(torch.randn(n_players).abs_().requires_grad_()))
        
    def get_strategy(self):
        strategies = torch.zeros((self.n_players, self.action_dim))
        for i in range(self.n_players):
            strategies[i] = Beta(self.alpha[i].exp(), self.beta[i].exp()).sample().requires_grad_()
        return strategies

    def payoff(self, action, rotated_action):
        x = action
        y = rotated_action
        #payoff = (1 + x) * (1 + y) * (1 - x * y) / (1 + x * y)**2
        ones = torch.ones_like(x)
        one_plus_x = torch.add(ones, x)
        one_plus_y = torch.add(ones, y)
        one_minus_xy = torch.sub(ones, torch.mul(x, y))
        one_plus_xy = torch.add(ones, torch.mul(x, y))
        
        numerator = torch.mul(torch.mul(one_plus_x, one_plus_y), one_minus_xy)
        denominator = torch.square(one_plus_xy)
        
        payoff = torch.div(numerator, denominator)
        
        return payoff
        

    def forward(self, actions):
        rotated_actions = torch.cat((actions[:, 1:], actions[:, :1]), dim=1)
        rewards = self.payoff(actions, rotated_actions)
        rewards[:,-1] *= -1
        return rewards

    def init_actions(self, n_ensembles):
        actions = torch.zeros((n_ensembles, self.n_players, self.action_dim))
        for i in range(n_ensembles):
            actions[i] = self.get_strategy()
        return actions
    def mix(self, strategy, epsilon):
        return (1 - epsilon) * strategy.max(dim=1).values + epsilon * strategy.sum(dim=1)

    def concater(self, strategies, action_samples, i, j):
        return torch.cat((strategies[:i], action_samples[j][i].unsqueeze(0), strategies[i+1:]), dim=0)

   
def train(game,n_iter=1000, lr=1e-3, eps=1e-3, n_ensembles=500):
    strategies = game.get_strategy().requires_grad_()
    strategies = torch.nn.Parameter(strategies)
    action_samples = game.init_actions(n_ensembles).requires_grad_()
    action_samples = torch.nn.Parameter(action_samples)
    optimizer_strat = torch.optim.Adam([strategies], lr=lr)
    optimizer_action = torch.optim.Adam([action_samples], lr=lr)
    scheduler_strat = torch.optim.lr_scheduler.StepLR(optimizer_strat, step_size=config['hyperparameters']['step_size'], gamma=config['hyperparameters']['gamma'])
    scheduler_action = torch.optim.lr_scheduler.StepLR(optimizer_action, step_size=config['hyperparameters']['step_size'], gamma=config['hyperparameters']['gamma'])
    iter_exploitabilities = []
    logger=utils.Logger(['Exploitability'])
    progress_bar = tqdm(range(n_iter), desc='Training', unit='iter', unit_scale=True)
    # for j in range(n_iter):
    #     optimizer_action.zero_grad()
    #     optimizer_strat.zero_grad()
    #     reward_strategy = game.forward(strategies)
    #     mix_strategy = torch.zeros(game.n_players)
    #     max_strategy = torch.zeros(game.n_players)
    #     for i in range(game.n_players):
    #         ensemble_reward = torch.zeros(n_ensembles)
    #         for k in range(n_ensembles):
    #             temp_tensor = game.concater(strategies, action_samples, i, k).requires_grad_()
    #             ensemble_reward[k] = game.payoff(temp_tensor, i)
    #         max_strategy[i] = ensemble_reward.max()
    #         mix_strategy[i] = game.mix(ensemble_reward, eps)
    #     loss_max = torch.sum(torch.abs(max_strategy - reward_strategy))
    #     loss_mix = torch.sum(torch.abs(mix_strategy - reward_strategy.detach()))
    #     loss_max.backward(retain_graph=True)
    #     loss_mix.backward()
    #     optimizer_strat.step()
    #     optimizer_action.step()
    #     scheduler_strat.step()
    #     scheduler_action.step()
    #     iter_exploitabilities.append([loss_max.item(),loss_mix.item()])
    #     logger.lossLog({'Exploitability':loss_max.item()})
    #     progress_bar.set_postfix({'time': time.time() - progress_bar.start_t, 'loss_max': loss_max.item()})
    #     # print(f"iter: {:.4d}, loss_max: {loss_max.item():.4f}, loss_mix: {loss_mix.item():.4f},strategies: {strategies.detach().numpy()}")
    for j in progress_bar:
        optimizer_action.zero_grad()
        optimizer_strat.zero_grad()
        strategies_ = torch.sigmoid(strategies)
        action_samples_ = torch.sigmoid(action_samples)
        reward_strategy = game.forward(strategies_).to(device)
        ensemble_reward = torch.zeros((game.n_players,n_ensembles,game.action_dim), device=device)
        for i in range(game.n_players):
            repeated_strategies = strategies_.repeat(n_ensembles, 1, 1)
            repeated_strategies[:, i, :] = action_samples_[:, i, :]
            ensemble_reward[i] = game.forward(repeated_strategies)[:,i,:]
        max_strategy = ensemble_reward.max(dim=1).values.to(device)
        mix_strategy = game.mix(ensemble_reward, eps)
        loss_max = - torch.mean(torch.abs(max_strategy - reward_strategy))
        loss_mix = torch.mean(torch.abs(mix_strategy - reward_strategy.detach()))
        (loss_max + loss_mix).backward()
        loss_max = - loss_max.detach()
        optimizer_strat.step()
        optimizer_action.step()
        scheduler_strat.step()
        scheduler_action.step()
        iter_exploitabilities.append([loss_max.item(),loss_mix.item()])
        logger.lossLog({'Exploitability': loss_max.item()})
        progress_bar.set_postfix({'time': time.time() - progress_bar.start_t, 'loss_max': loss_max.item()})
    return torch.sigmoid(strategies), iter_exploitabilities


# import config.yaml to set hyperparameters

with open('config.yaml') as f:
    config = yaml.load(f, Loader=yaml.FullLoader)
torch.manual_seed(config['hyperparameters']['seed'])
np.random.seed(config['hyperparameters']['seed'])
n_iter = config['hyperparameters']['epochs']
lr = config['hyperparameters']['learning_rate']
n_ensembles = config['hyperparameters']['n_ensembles']
eps = config['hyperparameters']['eps']
players = config['hyperparameters']['players']
       
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)
t = time.time()
game = GlicksbergGrossGame(n_players=players).to(device)
t = time.time() - t
strategies, exploitabilities = train(game, n_iter=n_iter, lr=lr, eps =eps, n_ensembles=n_ensembles)

# print strategies
print(f"strategies: {strategies.detach().numpy()}")

max_loss = [x[0] for x in exploitabilities]
mix_loss = [x[1] for x in exploitabilities]


plt.figure(figsize=(10, 5))


plt.subplot(1, 2, 1)
plt.title("Exploitability Max Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.plot(max_loss)



plt.subplot(1, 2, 2)
plt.title("Mix Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.plot(mix_loss)

name = f"GlicksbergGrossGame {n_iter} {players} {n_ensembles} {lr} {eps}".replace(".", "_")
plt.savefig(f"Results/rpaper/{name}.png")

# save results
with open(f"Results/rpaper/{name}.txt", "w") as f:
    # write config
    f.write(f"config: {config}\n")
    f.write(f"max_loss: {max_loss[-1]}\n")
    f.write(f"mix_loss: {mix_loss[-1]}\n")
    f.write(f"strategies: {strategies.detach().numpy()}")
    f.write(f"Time taken: {t:.4f} seconds\n")