import torch
import torch.nn as nn

class Player(nn.Module):
    def __init__(self):
        super(Player, self).__init__()
        self.strategy = nn.Parameter(torch.tensor([0.5, 0.5], requires_grad=True))
        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.1)
        self.payoff = torch.tensor(0.0, requires_grad=True)
    
    def get_action(self):
        self.probs = torch.softmax(self.strategy, dim=-1)
        self.action_probs = torch.distributions.Categorical(probs=self.probs)
        self.action = self.action_probs.sample()
        return self.action
    
    def update_strategy(self, loss):
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()


class MatchingPennies(nn.Module):
    def __init__(self, n_players):
        super(MatchingPennies, self).__init__()
        self.n_players = n_players
        # initialize players
        self.players = nn.ModuleList([Player() for _ in range(n_players)])
        self.action_space = [0, 1]

    def payoff(self, actions):
        for i in range(self.n_players):
            if actions[i] == actions[(i+1)%self.n_players]:
                self.players[i].payoff = torch.tensor(1.0, requires_grad=True)
            else:
                self.players[i].payoff = torch.tensor(-1.0, requires_grad=True)
            # create a dependency between payoff and actions
            #TODO:
        

        
        


mp5=MatchingPennies(5)
n_ensembles = 10

def train(mp, num_iterations):
    losses = [[] for _ in range(mp.n_players)]
    for iteration in range(num_iterations):
        for i in range(mp.n_players):
            actions = [mp.players[j].get_action() for j in range(mp.n_players)]
            mp.payoff(actions)
            max_payoff = max([mp.players[i].payoff for i in range(mp.n_players)])
            loss = -mp.players[i].payoff + max_payoff
            mp.players[i].update_strategy(loss)
            loss1 = loss.detach().numpy()
            losses[i].append(loss1)
            for j in range(mp.n_players):
                if j != i:
                    mp.players[j].update_strategy(-loss)
        print(f'iteration {iteration}')
        #print the strategy of each player
        for i in range(mp.n_players):
            print(f'player {i} strategy: {mp.players[i].strategy}')
            print(f'player {i} payoff: {mp.players[i].payoff}')
            print(f'player {i} action: {mp.players[i].action}')
            print(f'player {i} probs: {mp.players[i].probs}')
    return losses
             
# def train(mp, num_iterations):
#     losses = [[] for _ in range(mp.n_players)]
#     for i in range(mp.n_players):#X
#         max_payoffs = [[] for _ in range(mp.n_players)]
#         for iteration in range(num_iterations):#j
#             actions = [mp.players[j].get_action() for j in range(mp.n_players)]
#             mp.payoff(actions)
#             for j in range(mp.n_players):
#                 max_payoffs[j].append(mp.players[j].payoff)
#         max_payoffs = torch.stack(max_payoffs)
#         max_payoffs = torch.max(max_payoffs, dim=1)


        

        
            




    #         max_payoff = max([mp.players[j].payoff for j in range(mp.n_players) and i!=j])

    #         loss = -mp.players[i].payoff + max_payoff
    #         mp.players[i].update_strategy(loss)
    #         loss = loss.detach().numpy()
    #         losses[i].append(loss)
    #         for j in range(mp.n_players):
    #             if j != i:
    #                 mp.players[j].update_strategy(-loss)
    # return losses
             


loss = train(mp5, 10)

# plot the losses
import matplotlib.pyplot as plt
for i in range(mp5.n_players):
    plt.plot(loss[i], label=f'player {i}')
plt.legend()
plt.show()

    


    

    


    

    

        

# class BestResponseEnsemble(nn.Module):


# 

# if __name__ == "__main__":



