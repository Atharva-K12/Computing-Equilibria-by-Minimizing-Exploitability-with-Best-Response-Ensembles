import torch


class Player:
    def __init__(self):
        self.strategy = torch.zeros(2, requires_grad=True)
        self.optimizer = torch.optim.SGD([self.strategy], lr=0.1)
        self.probs = None
        self.action = None
        self.payoff = torch.tensor(0.0, requires_grad=True)
    
    def get_action(self):
        self.probs = torch.softmax(self.strategy, dim=-1)
        self.action = torch.multinomial(self.probs, 1)
        return self.action.item()
    
    def update_strategy(self, loss):
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
    

class MatchingPennies:
    def __init__(self, n_players):
        self.n_players = n_players
        # initialize players
        self.players = [Player() for _ in range(n_players)]
        self.action_space = [0, 1]

    def payoff(self, a):
        for i in range(self.n_players):
            if a[i] == a[(i+1)%self.n_players]:
                self.players[i].payoff = torch.tensor(1.0, requires_grad=True)
            else:
                self.players[i].payoff = torch.tensor(-1.0, requires_grad=True)
        # if a[self.n_players-1] == a[0]:
        #     self.players[self.n_players-1].payoff = torch.tensor(1)
        # else:
        #     self.players[self.n_players-1].payoff = torch.tensor(-1)


class BestResponseEnsemble:
    def __init__(self, n_players, learning_rate=0.1):
        self.n_players = n_players
        self.matching_pennies = MatchingPennies(n_players)
        self.learning_rate = learning_rate

    def train(self, num_iterations):
        losses = [[] for _ in range(self.n_players)]
        for iteration in range(num_iterations):
            actions = [self.matching_pennies.players[i].get_action() for i in range(self.n_players)]
            self.matching_pennies.payoff(actions)
            for i in range(self.n_players):
                loss = -self.matching_pennies.players[i].payoff
                self.matching_pennies.players[i].update_strategy(loss)
                loss = loss.detach().numpy()
                losses[i].append(loss)
        # print("strategy: ", self.matching_pennies.players[0].strategy)
        # print("probs: ", self.matching_pennies.players[0].probs)
        # print("loss: ", loss)
        # print("payoffs: ", self.matching_pennies.players[0].payoff)
        actions = [self.matching_pennies.players[i].get_action() for i in range(self.n_players)]
        print("actions: ", actions)
        for i in range(self.n_players):
            print("player ", i, " strategy: ", self.matching_pennies.players[i].strategy)
            print("player ", i, " probs: ", self.matching_pennies.players[i].probs)
            print("player ", i, " loss: ", loss)
            print("player ", i, " payoffs: ", self.matching_pennies.players[i].payoff)
            print("player ", i, " action: ", self.matching_pennies.players[i].action)
        #plot losses
        import matplotlib.pyplot as plt
        for i in range(self.n_players):
            plt.plot(losses[i], label="player "+str(i))
        plt.show()
        return loss

        
    
if __name__ == '__main__':
    bre = BestResponseEnsemble(6)
    print(bre.train(100))
    


    

# class MatchingPennies:
#     def __init__(self, n_players):
#         self.n_players = n_players
#         self.action_space = [0, 1]
        
#     #payoff defined as u(a)i = [ai = ai+1] for i < n and [an != a1] for i = n,
#     #where [Â·] denotes the Iverson bracket, which is 1 if its argument is true and 0 otherwise
#     #a is tensor of actions
#     def payoff(self, a):
#         payoffs = torch.zeros(self.n_players)
#         for i in range(self.n_players):
#             payoffs[i] = (a[i] == a[(i+1)%self.n_players]).float()
#         payoffs[self.n_players-1] = (a[self.n_players-1] != a[0]).float()
#         return payoffs
    
# class Util:
#     def mix(a,epsilon):
#         return (1-epsilon)*a.max() + epsilon*a.sum()


# class BRE:
#     def __init__(self, n_players, learning_rate=0.1):
#         self.n_players = n_players
#         self.matching_pennies = MatchingPennies(n_players)
#         self.strategy = torch.ones(self.n_players, len(self.matching_pennies.action_space), requires_grad=True)
#         self.optimizer = torch.optim.SGD([self.strategy], lr=learning_rate)
#         self.probs = None
    
#     def _get_action(self, player):
#         self.probs = torch.softmax(self.strategy[player], dim=-1)
#         action = torch.multinomial(self.probs, 1)
#         print("action: ", action)
#         return action.item()
    
#     def train(self, num_iterations):
#         for iteration in range(num_iterations):
#             actions = [self._get_action(player) for player in range(self.n_players)]
#             print("actions: ", actions)
#             payoffs = self.matching_pennies.payoff(torch.tensor(actions))#,requires_grad=True))
#             print("payoffs: ", payoffs)
#             self.optimizer.zero_grad()
#             loss = -payoffs
#             loss.backward()
#             self.optimizer.step()
#             print("strategy: ", self.strategy)
#             print("probs: ", self.probs)
#             print("loss: ", loss)
#             print("payoffs: ", payoffs)


    
#     def get_strategy(self):
#         return self.strategy
    

# if __name__ == "__main__":
#     bre = BRE(6)
#     exploitability = bre.train(100)
#     print("exploitability: ", exploitability)
#     print("strategy: ", bre.get_strategy())


