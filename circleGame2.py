import torch
import numpy as np
from scipy.spatial.distance import pdist, squareform


class CircleGame(torch.nn.Module):
    def __init__(self, n_players):
        super().__init__()
        self.n_players = n_players
        self.action_dim = 1
        self.action_space = torch.linspace(-np.pi, np.pi, 100)
        self.register_parameter('x', torch.nn.Parameter(torch.randn(n_players, 2)))
        self.register_parameter('log_s', torch.nn.Parameter(torch.randn(n_players)))
        
    def get_action(self):
        with torch.no_grad():
            mus = torch.atan2(self.x[:, 1], self.x[:, 0])
            sigmas = torch.exp(self.log_s)
            actions = torch.zeros(self.n_players, self.action_dim)
            for i in range(self.n_players):
                actions[i] = torch.distributions.von_mises.VonMises(mus[i], sigmas[i]).sample()
            return actions
        
    def pairwise_distances(self, actions):
        diffs = actions.unsqueeze(0) - actions.unsqueeze(1)
        return torch.min(torch.abs(diffs), 2 * np.pi - torch.abs(diffs))
        
    def forward(self, actions):
        dists = self.pairwise_distances(actions)
        rewards = -dists[:, :-1] + dists[:, -1:]
        return rewards


# def exploitability(game, strategies):
#     n_samples = 100
#     actions = torch.zeros(game.n_players, game.action_dim)
#     values = []
#     for i in range(game.n_players):
#         opponent_strategies = torch.cat((strategies[:i], strategies[i+1:]), dim=0).unsqueeze(0)
#         # expected_rewards = torch.zeros(n_samples)
#         for j in range(n_samples):
#             actions[i] = torch.distributions.von_mises.VonMises(0, 1).sample()
#             expected_rewards = 0
#             for opponent_action in opponent_strategies:
#                 tempTensor = opponent_action.unsqueeze(0)
#                 if i != 0:
#                     tempTensor = torch.cat((actions[:i], tempTensor), dim=0)
#                 if i != game.n_players - 1:
#                     tempTensor = torch.cat((tempTensor, actions[i+1:]), dim=0)
#                 expected_rewards += game.forward(tempTensor).max()
#             values.append(expected_rewards.max())
#     return np.mean(values)
        
        
# def best_response_ensemble(game, strategies, n_samples):
#     for i in range(game.n_players):
#         opponent_strategies = torch.cat((strategies[:i], strategies[i+1:]), dim=0).unsqueeze(0)
#         action_samples = torch.zeros((n_samples, game.action_dim))
#         expected_rewards = torch.zeros(n_samples)
#         for j in range(n_samples):
#             action_samples[j] = torch.distributions.von_mises.VonMises(0, 1).sample()
#             for opponent_action in opponent_strategies:
#                 tempTensor = opponent_action.unsqueeze(0)
#                 if j != 0:
#                     tempTensor = torch.cat((action_samples[:j], tempTensor), dim=0)
#                 if j != n_samples - 1:
#                     tempTensor = torch.cat((tempTensor, action_samples[j+1:]), dim=0)
#                 expected_rewards[j] += game.forward(tempTensor).max()

#         if i < game.n_players - 1:
#             best_idx = expected_rewards.argmax()
#         else:
#             best_idx = expected_rewards.argmin()
        
#         strategies[i] = action_samples[best_idx]
#     return strategies

def mix(strategy, epsilon):
    return (1 - epsilon) * strategy.max() + epsilon * strategy.sum()

    
def train(game, strategies,  n_iter=1000, lr=1e-3, eps=1e-3):
    last_reward = torch.zeros(game.n_players)
    exploitabilities = []
    action_samples = [torch.zeros((game.n_players - 1, game.action_dim)) for _ in range(game.n_players)]
    for _ in range(n_iter):
        iter_exploitabilities = []
        for i in range(game.n_players):
            # opponent_strategies = torch.cat((strategies[:i], strategies[i+1:]), dim=0)
            # print("opponent_strategies ", opponent_strategies, opponent_strategies.shape)
            expected_rewards = torch.zeros(game.n_players - 1)
            for j in range(game.n_players - 1):
                # Find best action for player i with respect to opponent j
                # tempTensor = opponent_strategies
                # print("strategy ", strategies[:i], strategies[:i].shape)
                # print("strategy ", strategies[i+1:], strategies[i+1:].shape)
                # print("Y",i,j, " ", action_samples[i][j], action_samples[i][j].shape)
                # print("Y",i,j, " ", action_samples[i][j].unsqueeze(0), action_samples[i][j].unsqueeze(0).shape)
                tempTensor = torch.cat((strategies[:i], action_samples[i][j].unsqueeze(0),strategies[i+1:]),dim=0)
                # print("tempTensor ", tempTensor, tempTensor.shape)
                # if j != 0:
                #     tempTensor = torch.cat((action_samples[i][:j], tempTensor), dim=0)
                # if j != game.n_players - 2:
                #     tempTensor = torch.cat((tempTensor, action_samples[i][j+1:]), dim=0)
                # expected_rewards[j] += game.forward(tempTensor).max() in dimension 1
                reward = game.forward(tempTensor)
                # print("print ",reward, reward.shape)
                expected_rewards[j] += reward.max()
            best_idx = expected_rewards.argmax()
            # find differentiation of expected reward wrt strategy of player i
            # update strategy of player i

            print("Expected rewards: ",expected_rewards[best_idx], last_reward[i], expected_rewards[best_idx] - last_reward[i])
            strategies[i] -= lr * (expected_rewards[best_idx] - last_reward[i])
            if strategies[i] > np.pi:
                strategies[i] = np.pi - strategies[i]
            if strategies[i] < -np.pi:
                strategies[i] = np.pi + strategies[i]
            mix_strategy = mix(action_samples[i], eps)
            for j in range(game.n_players - 1):
                action_samples[i][j] -= lr * (mix_strategy - last_reward[j])
                if action_samples[i][j] > np.pi:
                    action_samples[i][j] = np.pi - action_samples[i][j]
                if action_samples[i][j] < -np.pi:
                    action_samples[i][j] = np.pi + action_samples[i][j]
            iter_exploitabilities.append(last_reward[i] - expected_rewards[best_idx])
            last_reward[i] = expected_rewards[best_idx]
        # Save sum of iter exploitabilities in exploitabilities
        exploitabilities.append(float(sum(iter_exploitabilities)))
        print("Exploitability: ", exploitabilities[-1])
    return strategies, exploitabilities

            

       
game = CircleGame(n_players=4)
strategies = game.get_action()
print("Strategies: ")
print(strategies)
strategies, exploitabilities = train(game, strategies, n_iter=100, lr=1e-2, eps=1e-1)
print("Strategies: ")
print(strategies)
print(exploitabilities)
# plot exploitabilities which is a list of tensor
import matplotlib.pyplot as plt
plt.plot(exploitabilities)
plt.show()

# exp_list = []
# for i in range(1):
#     strategies = best_response_ensemble(game, strategies, n_samples=3)
#     exp = exploitability(game, strategies)
#     print("strategies: ")
#     print(strategies)
#     exp_list.append(exp)
#     if i % 1 == 0:
#         print(f'Iteration {i}, exploitability: {exp}')


# plot exploitability
# import matplotlib.pyplot as plt
# plt.plot(exp_list)
# plt.show()
