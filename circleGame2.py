import torch
import numpy as np
from scipy.spatial.distance import pdist, squareform


class CircleGame(torch.nn.Module):
    def __init__(self, n_players):
        super().__init__()
        self.n_players = n_players
        self.action_dim = 1
        self.action_space = torch.linspace(-np.pi, np.pi, 100)
        self.register_parameter('x', torch.nn.Parameter(torch.randn(n_players, 2)))
        self.register_parameter('log_s', torch.nn.Parameter(torch.randn(n_players)))
        
    def get_action(self):
        with torch.no_grad():
            mus = torch.atan2(self.x[:, 1], self.x[:, 0])
            sigmas = torch.exp(self.log_s)
            actions = torch.zeros(self.n_players, self.action_dim)
            for i in range(self.n_players):
                actions[i] = torch.distributions.von_mises.VonMises(mus[i], sigmas[i]).sample()
            return actions
        
    def pairwise_distances(self, actions):
        diffs = actions.unsqueeze(0) - actions.unsqueeze(1)
        return torch.min(torch.abs(diffs), 2 * np.pi - torch.abs(diffs))
        
    def forward(self, actions):
        dists = self.pairwise_distances(actions)
        rewards = -dists[:, :-1] + dists[:, -1:] 
        return rewards


def mix(strategy, epsilon):
    return (1 - epsilon) * strategy.max() + epsilon * strategy.sum()

    
def train(game, strategies,  n_iter=1000, lr=1e-3, eps=1e-3):
    last_reward = torch.zeros(game.n_players)
    for i in range(game.n_players):
        last_reward[i] = game.forward(strategies).max()
    exploitabilities = []
    action_samples = [torch.zeros((game.n_players - 1, game.action_dim)) for _ in range(game.n_players)]
    print("action_samples: ", len(action_samples), action_samples[0].shape)
    for _ in range(n_iter):
        iter_exploitabilities = []
        for i in range(game.n_players):
            expected_rewards = torch.zeros(game.n_players - 1)
            for j in range(game.n_players - 1):
                tempTensor = torch.cat((strategies[:i], action_samples[i][j].unsqueeze(0),strategies[i+1:]),dim=0)
                reward = game.forward(tempTensor)
                expected_rewards[j] += reward.max()
            best_idx = expected_rewards.argmax()
            print("Expected rewards: ",expected_rewards[best_idx], last_reward[i], expected_rewards[best_idx] - last_reward[i])
            strategies[i] -= lr * (expected_rewards[best_idx] - last_reward[i])
            strategies[i] = torch.clamp(strategies[i], -np.pi, np.pi)
            mix_strategy = mix(action_samples[i], eps)
            for j in range(game.n_players - 1):
                action_samples[i][j] -= lr * (mix_strategy - last_reward[j])
                action_samples[i][j] = torch.clamp(action_samples[i][j], -np.pi, np.pi)
            iter_exploitabilities.append(last_reward[i] - expected_rewards[best_idx])
            last_reward[i] = expected_rewards[best_idx]
        # Save sum of iter exploitabilities in exploitabilities
        if sum(iter_exploitabilities) == 0:
            break
        exploitabilities.append(float(sum(iter_exploitabilities)))
        print("Exploitability: ", exploitabilities[-1])
    return strategies, exploitabilities


# def train(game, strategies,  n_iter=1000, lr=1e-3, eps=1e-3):
#     last_reward = torch.zeros(game.n_players)
#     for i in range(game.n_players):
#         last_reward[i] = game.forward(strategies).max()
#     exploitabilities = []
#     action_samples = [torch.zeros((game.n_players - 1, game.action_dim)) for _ in range(game.n_players)]
#     for _ in range(n_iter):
#         iter_exploitabilities = []
#         for i in range(game.n_players):
#             expected_rewards = torch.zeros(game.n_players - 1)
#             for j in range(game.n_players - 1):
#                 tempTensor = torch.cat((strategies[:i], action_samples[i][j].unsqueeze(0),strategies[i+1:]),dim=0)
#                 reward = game.forward(tempTensor)
#                 expected_rewards[j] += reward.max()
#             best_idx = expected_rewards.argmax()
#             print("Expected rewards: ",expected_rewards[best_idx], last_reward[i], expected_rewards[best_idx] - last_reward[i])
#             strategies[i] -= lr * (expected_rewards[best_idx] - last_reward[i])
#             strategies[i] = torch.clamp(strategies[i], -np.pi, np.pi)
#             mix_strategy = mix(action_samples[i], eps)
#             for j in range(game.n_players - 1):
#                 action_samples[i][j] -= lr * (mix_strategy - last_reward[j])
#                 action_samples[i][j] = torch.clamp(action_samples[i][j], -np.pi, np.pi)
#             iter_exploitabilities.append(last_reward[i] - expected_rewards[best_idx])
#             last_reward[i] = expected_rewards[best_idx]
#         # Save sum of iter exploitabilities in exploitabilities
#         if sum(iter_exploitabilities) == 0:
#             break
#         exploitabilities.append(float(sum(iter_exploitabilities)))
#         print("Exploitability: ", exploitabilities[-1])
#     return strategies, exploitabilities


            

       
game = CircleGame(n_players=4)
strategies = game.get_action()
print("Strategies: ")
print(strategies)
strategies, exploitabilities = train(game, strategies, n_iter=1, lr=0.1, eps=0.5)
print("Strategies: ")
print(strategies)
print(exploitabilities)
# plot exploitabilities which is a list of tensor
import matplotlib.pyplot as plt
plt.plot(exploitabilities)
plt.show()

# exp_list = []
# for i in range(1):
#     strategies = best_response_ensemble(game, strategies, n_samples=3)
#     exp = exploitability(game, strategies)
#     print("strategies: ")
#     print(strategies)
#     exp_list.append(exp)
#     if i % 1 == 0:
#         print(f'Iteration {i}, exploitability: {exp}')


# plot exploitability
# import matplotlib.pyplot as plt
# plt.plot(exp_list)
# plt.show()
